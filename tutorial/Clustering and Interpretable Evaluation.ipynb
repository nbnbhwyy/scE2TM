{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Evaluation (ARI and NMI)\n",
    "We use ARI and NMI to evaluate embedding clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'Wang'  # Set dataset name\n",
    "\n",
    "# Read data and create AnnData object\n",
    "# Load embedding representation data from CSV file, construct file path using dataset name\n",
    "adata = sc.AnnData(pd.read_csv('../output/'+dataname+'/'+dataname+'_embedding.csv',sep=',',index_col=0))\n",
    "\n",
    "# Load cell annotation information\n",
    "label = pd.read_csv('../data/'+dataname+'_cell_anno.csv',sep=',',index_col=0)\n",
    "adata.obs['cell_type'] = list(label[dataname.lower()+\"@colData$cell_type1\"])\n",
    "\n",
    "# Builds neighborhood graph for umap\n",
    "sc.pp.neighbors(adata,use_rep='X')\n",
    "\n",
    "# Set resolution parameter range for Louvain clustering\n",
    "# Will test resolutions from minn to maxn in increments of 0.1\n",
    "maxn = 2\n",
    "minn= 0\n",
    "list_value = []\n",
    "for x in  range(minn, maxn*10):\n",
    "    sc.tl.louvain(adata,resolution=x/10.0,random_state=0)\n",
    "    list_value.append(adjusted_rand_score(adata.obs['cell_type'],adata.obs['louvain']))\n",
    "sc.tl.louvain(adata,resolution=list_value.index(max(list_value))*0.1,random_state=0)\n",
    "print(list_value.index(max(list_value))*0.1)\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "# Visualize UMAP with both true cell types and Louvain clusters\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[\"cell_type\",\"louvain\"],\n",
    "    wspace = 0.3,\n",
    "    frameon=False,\n",
    "    #save = \"scE2TM_cluster_\"+dataname+\".pdf\"\n",
    ")\n",
    "print(\"scE2TM  Adjusted_rand_score   \"+str(adjusted_rand_score(adata.obs['cell_type'],adata.obs['louvain']))+\"   Adjusted_mutual_info_score   \"+str(adjusted_mutual_info_score(adata.obs['cell_type'],adata.obs['louvain'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Evaluation\n",
    "We assess single-cell embedding topic model interpretability using the proposed metrics: IP, TC, TD, TQ, ORA<sub>N</sub>, ORA<sub>U</sub>,\n",
    "ORA<sub>Q</sub>, GSEA<sub>N</sub>, GSEA<sub>U</sub>, and GSEA<sub>Q</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gseapy as gp\n",
    "import numpy as np\n",
    "\n",
    "# Use gseapy to load the KEGG gene set from the GMT file as the background dataset (bg_data) when calculating TC.\n",
    "kegg = gp.read_gmt(path=\"../data/msigdb.v2024.1.Hs.symbols.gmt\") \n",
    "gene_set = []\n",
    "cell_gene = []\n",
    "for value in kegg.values():\n",
    "    gene_set.extend(value)\n",
    "    cell_gene.append(value)\n",
    "gene_set = list(set(gene_set))\n",
    "dicts_gene_index = {}\n",
    "for index, value in enumerate(gene_set):\n",
    "    dicts_gene_index[value] = index\n",
    "bg_data = np.zeros((len(kegg),len(gene_set)))\n",
    "for index, values in enumerate(kegg.values()):\n",
    "    for value in values:\n",
    "        bg_data[index][dicts_gene_index[value]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tg = pd.read_csv('../output/'+dataname+'/'+dataname+'_tg.csv',sep=',',index_col=0) #Load topic-gene matrix\n",
    "data_exp = pd.read_csv('../data/'+dataname+'_HIGHPRE_5000.csv',sep=',',index_col=0)\n",
    "data_tg.columns = data_exp.columns #Extract the gene list from the corresponding single-cell data.\n",
    "data_tg = data_tg.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(doc_gene, topic_gene, N, dicts_gene_tran):\n",
    "    \"\"\"\n",
    "    Compute Topic Coherence (TC) metric for topic models.\n",
    "    TC measures the semantic coherence of topics based on co-occurrence statistics\n",
    "    of top genes in the background corpus.\n",
    "    \"\"\"\n",
    "    # print('computing coherence ...')    \n",
    "    topic_size, gene_size = np.shape(topic_gene)\n",
    "    doc_size = np.shape(doc_gene)[0]\n",
    "    # find top genes'index of each topic\n",
    "    topic_list = []\n",
    "    for topic_idx in range(topic_size):\n",
    "        top_gene_idx = np.argpartition(topic_gene[topic_idx, :], -N)[-N:]\n",
    "        topic_list.append(top_gene_idx)\n",
    "    #print(topic_list)\n",
    "    # compute coherence of each topic\n",
    "    sum_coherence_score = 0.0\n",
    "    for i in range(topic_size):\n",
    "        gene_array = topic_list[i]\n",
    "        sum_score = 0.0\n",
    "        for n in range(N):\n",
    "            if gene_array[n] in dicts_gene_tran:\n",
    "                flag_n = doc_gene[:, dicts_gene_tran[gene_array[n]]] > 0\n",
    "                p_n = np.sum(flag_n) / doc_size\n",
    "                for l in range(n + 1, N):\n",
    "                    if gene_array[l] in dicts_gene_tran:\n",
    "                        flag_l = doc_gene[:, dicts_gene_tran[gene_array[l]]] > 0\n",
    "                        p_l = np.sum(flag_l)\n",
    "                        p_nl = np.sum(flag_n * flag_l)\n",
    "                        if p_n * p_l * p_nl > 0:\n",
    "                            p_l = p_l / doc_size\n",
    "                            p_nl = p_nl / doc_size\n",
    "                            sum_score += np.log(p_nl / (p_l * p_n)) / -np.log(p_nl)\n",
    "        sum_coherence_score += sum_score * (2 / (N * N - N))\n",
    "    sum_coherence_score = sum_coherence_score / topic_size\n",
    "    return sum_coherence_score\n",
    "\n",
    "# Create index mapping dictionary\n",
    "# Maps indices from topic-gene matrix to background corpus matrix indices\n",
    "dicts_gene_tran = {}\n",
    "for index, value in enumerate(data_tg.index):\n",
    "    if value in dicts_gene_index:\n",
    "        dicts_gene_tran[index] = dicts_gene_index[value]\n",
    "\n",
    "TC = compute_coherence(bg_data, data_tg.T.values, 10, dicts_gene_tran)\n",
    "\n",
    "print(f\"===>TC_T{10}: {TC:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def TD_eva(texts):\n",
    "    \"\"\"\n",
    "    Calculate Topic Distinctiveness (TD) metric.\n",
    "    TD measures the proportion of unique words that appear in only one topic.\n",
    "    Higher TD indicates better topic distinctiveness (less overlap between topics).\n",
    "    \"\"\"\n",
    "    K = len(texts)\n",
    "    T = len(texts[0].split())\n",
    "    vectorizer = CountVectorizer()\n",
    "    counter = vectorizer.fit_transform(texts).toarray()\n",
    "    TF = counter.sum(axis=0)\n",
    "    print(counter)\n",
    "    TD = (TF == 1).sum() / (K * T)\n",
    "    return TD\n",
    "\n",
    "def ext_topic_genes(beta, vocab, num_top_gene):\n",
    "    \"\"\"\n",
    "    Extract top genes for each topic from topic-gene distribution matrix.\n",
    "    \"\"\"\n",
    "    topic_str_list = list()\n",
    "    for i, topic_dist in enumerate(beta):\n",
    "        topic_genes = np.array(vocab)[np.argsort(topic_dist)][:-(num_top_gene + 1):-1]\n",
    "        topic_str = ' '.join(topic_genes)\n",
    "        topic_str_list.append(topic_str)\n",
    "    return topic_str_list\n",
    "\n",
    "topic_str_list = ext_topic_genes(data_tg.T.values, data_tg.index, 10)\n",
    "TD = TD_eva(topic_str_list)\n",
    "print(f\"===>TD_T{10}: {TD:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TQ = TC*TD\n",
    "print(f\"===>TQ_T{10}: {TQ:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "print(purity_score(adata.obs['cell_type'],np.argmax(adata.X, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORA<sub>N</sub>, ORA<sub>U</sub>, and ORA<sub>Q</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sets = gp.read_gmt(path=\"../data/c2.all.v2024.1.Hs.symbols.gmt\")\n",
    "for key, values in (gene_sets.items()):\n",
    "    values = [val.upper() for val in values]\n",
    "    gene_sets[key] = values\n",
    "\n",
    "print('------------')\n",
    "print(len(gene_sets))\n",
    "print('------------')\n",
    "topic_str_list = ext_topic_genes(data_tg.T.values, data_tg.index, 10)\n",
    "all_res = None\n",
    "gene_set = copy.deepcopy(gene_sets)\n",
    "for index in range(len(data_tg.iloc[0])):\n",
    "    flag = False\n",
    "    data_temp = topic_str_list[index]\n",
    "    try:\n",
    "        pre_res = gp.enrich(gene_list=data_temp.split(), # or gene_list=glist\n",
    "                    gene_sets=gene_set, \n",
    "                    outdir=None,\n",
    "                    verbose=True)\n",
    "        pre_res.res2d.insert(0,'topic_index',index)\n",
    "        if index == 0:\n",
    "            all_res = pre_res.res2d\n",
    "        else:\n",
    "            all_res = all_res.append(pre_res.res2d)\n",
    "    except:\n",
    "        print('NAN')\n",
    "        \n",
    "all_res_table = all_res[all_res[\"Adjusted P-value\"] <=0.01]\n",
    "print(len(all_res_table))\n",
    "print(len(set(all_res_table['Term'])))\n",
    "print(len(set(all_res_table['Term']))/len(all_res_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSEA<sub>N</sub>, GSEA<sub>U</sub>, and GSEA<sub>Q</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "gene_sets = gp.read_gmt(path=\"../data/c2.all.v2024.1.Hs.symbols.gmt\")\n",
    "for key, values in (gene_sets.items()):\n",
    "    values = [val.upper() for val in values]\n",
    "    gene_sets[key] = values\n",
    "    \n",
    "print('------------')\n",
    "print(len(gene_sets))\n",
    "print('------------')\n",
    "all_res = None\n",
    "gene_set = copy.deepcopy(gene_sets)\n",
    "for index in range(len(data_tg.iloc[0])):\n",
    "    data_temp  = data_tg.iloc[:,index]\n",
    "    try:\n",
    "        pre_res = gp.prerank(rnk=data_temp, # or rnk = rnk,\n",
    "                        gene_sets=gene_set,#\n",
    "                        threads=100,\n",
    "                        outdir=None, # don't write to disk\n",
    "                        )\n",
    "        pre_res.res2d.insert(0,'topic_index',index)\n",
    "        if index == 0:\n",
    "            all_res = pre_res.res2d\n",
    "        else:\n",
    "            all_res = all_res.append(pre_res.res2d)\n",
    "    except:\n",
    "        print('NAN')\n",
    "        \n",
    "all_res_table = all_res[all_res[\"FDR q-val\"] <=0.01]\n",
    "print(len(all_res_table))\n",
    "print(len(set(all_res_table['Term'])))\n",
    "print(len(set(all_res_table['Term']))/len(all_res_table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scE2TM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
